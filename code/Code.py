# -*- coding: utf-8 -*-
"""genaiProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ByXsflzRCB7Ar4Gx-RaoxxYF5ZceqlaK
"""

!pip install transformers datasets scikit-learn spacy
!python -m spacy download en_core_web_sm

!pip install --upgrade transformers

!pip install indic-transliteration

pip install indic-transliteration transformers torch

pip install googletrans==4.0.0-rc1 indic-transliteration

!pip uninstall tensorflow tensorflow-gpu numpy -y

!pip install tensorflow==2.12.0

import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.models import Model
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

print("Setting up...")
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
tf.keras.backend.clear_session()

print("Loading data...")
data = pd.read_csv('/content/translatedDataset(Task_B).csv')
data = data.dropna(subset=['Tweet'])

def clean_text(tweet):
    if isinstance(tweet, str):
        tweet = tweet.lower()
        tweet = re.sub(r'http\S+|www\S+|https\S+', '', tweet)
        tweet = re.sub(r'@\w+', '', tweet)
        tweet = re.sub(r'[^\w\s]', '', tweet)
        tokens = word_tokenize(tweet)
        tokens = [w for w in tokens if w not in stopwords.words('english')]
        return " ".join(tokens)
    return ""

data['cleaned_text'] = data['Tweet'].apply(clean_text)
data = data[data['cleaned_text'].str.len() > 0]

data.loc[:, 'Hate'] = data['Hate'].astype(int)
valid_targets = {'O': 0, 'o': 0, 'I': 1, 'R': 2}
data.loc[:, 'Target'] = data['Target'].apply(lambda x: valid_targets.get(x, 0))
valid_severity = {'L': 0, 'M': 1, 'H': 2}
data.loc[:, 'Severity'] = data['Severity'].apply(lambda x: valid_severity.get(x, 0))

X = data['cleaned_text'].values
y_hate = data['Hate'].values
y_target = to_categorical(data['Target'].values, num_classes=3)
y_severity = to_categorical(data['Severity'].values, num_classes=3)

X_train, X_test, y_hate_train, y_hate_test, y_target_train, y_target_test, y_severity_train, y_severity_test = train_test_split(
    X, y_hate, y_target, y_severity, test_size=0.2, random_state=42
)

print("Loading Universal Sentence Encoder...")
embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")
X_train_emb = embed(X_train).numpy()
X_test_emb = embed(X_test).numpy()

#Customized model
print("Building model...")
input_layer = Input(shape=(512,))
dense = Dense(128, activation='relu')(input_layer)
dropout = Dropout(0.3)(dense)

output1 = Dense(1, activation='sigmoid', name='hate_output')(dropout)
output2 = Dense(3, activation='softmax', name='target_output')(dropout)
output3 = Dense(3, activation='softmax', name='severity_output')(dropout)

model = Model(inputs=input_layer, outputs=[output1, output2, output3])

model.compile(
    optimizer='adam',
    loss={
        'hate_output': 'binary_crossentropy',
        'target_output': 'categorical_crossentropy',
        'severity_output': 'categorical_crossentropy'
    },
    metrics={
        'hate_output': ['accuracy'],
        'target_output': ['accuracy'],
        'severity_output': ['accuracy']
    }
)

try:
    print("Training model...")
    history = model.fit(
        X_train_emb,
        {
            'hate_output': y_hate_train,
            'target_output': y_target_train,
            'severity_output': y_severity_train
        },
        validation_data=(
            X_test_emb,
            {
                'hate_output': y_hate_test,
                'target_output': y_target_test,
                'severity_output': y_severity_test
            }
        ),
        epochs=30,
        batch_size=32,
        verbose=1
    )
    print("Training completed successfully!")
except Exception as e:
    print(f"An error occurred during training: {e}")

!pip install joblib

import joblib

model.save('hate_speech_model.h5')
joblib.dump(model, 'hate_speech_model.pkl')

from google.colab import files
files.download('hate_speech_model.h5')
files.download('hate_speech_model.pkl')

hate_labels = {0: "Not Hate", 1: "Hate"}
target_labels = {0: "Other", 1: "Individual", 2: "Group"}
severity_labels = {0: "Low", 1: "Medium", 2: "High"}

def predict_hate_speech(sentence):
    cleaned_sentence = clean_text(sentence)
    if not cleaned_sentence:
        print("Input sentence is empty after cleaning.")
        return None

    sentence_emb = embed([cleaned_sentence]).numpy()

    predictions = model.predict(sentence_emb)
    hate_prediction = (predictions[0][0] > 0.5).astype(int)
    target_prediction = predictions[1].argmax(axis=1)
    severity_prediction = predictions[2].argmax(axis=1)

    hate_label = hate_labels[hate_prediction[0]]
    target_label = target_labels[target_prediction[0]]
    severity_label = severity_labels[severity_prediction[0]]

    return hate_label, target_label, severity_label

user_input = input("Enter a sentence for prediction: ")
hate, target, severity = predict_hate_speech(user_input)
print(f"Hate: {hate}, Target: {target}, Severity: {severity}")